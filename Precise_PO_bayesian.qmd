---
title: "**PRECISe trial: bayesian analysis**"
subtitle: '**Primary Outcome: EQ-5D-5L utility scores**'
author: "Andrea Gabrio"
date: ''
format: 
  html:
    code-fold: true
bibliography: report.bib
csl: apa.csl
---

## PRECISe trial

The PRECISe trial is an investigator-initiated pragmatic, binational multi-center, randomized controlled, quadruple-blinded study, designed to assess the effect of high protein enteral nutrition (target 2.0 g/kg/day) vs standard protein enteral nutrition (target 1.3 g/kg/day) on functional recovery at 30 days, 60 days, and 180 days following ICU admission, including health-related quality of life, measures of muscle strength, physical function, and mental. health. The trial's primary endpoint is health-related quality of life as measured by the Euro-QoL-5D-5-level (EQ-5D-5L) questionnaire Health Utility Score. Between-group differences of the primary and other sequential endpoints will be assessed over the three time points using linear mixed-effects models.

### Enrolment Criteria and interventions

Adult patients 18 years and above with an unplanned admission to the ICU, being mechanically ventilated within 24 h following ICU admission, and with an expected duration of mechanical ventilation of at least 3 days (i.e., indication for enteral nutrition support) were included. Exclusion criteria are; contraindication for enteral nutrition at the discretion of the treating physician, moribund or expected withholding of treatment, kidney failure without the possibility of dialysis, hepatic encephalopathy West Haven criteria 3e4, or a body mass index <18 kg/m2.

Patients were randomized in a 1:1 ratio, using permuted block randomization with varying blocks of 4 or 6 patients per center, into one of the two intervention groups. One group received enteral nutrition with a high protein content of 8g/100 kcal (expected intake 1,6e2,0 g/kg/day), and the other group received enteral nutrition with a standard protein content of 5g/100 kcal (expected intake 0,8e1,25 g/kg/day).

### Principles of Bayesian analysis

A cornerstone of Bayesian inference is the incorporation of prior beliefs about an effect estimate (the prior) into the calculation of the posterior probability of that effect estimate (the posterior), following the emergence of novel evidence (the likelihood). This methodology resembles clinical reasoning, where one's strong belief (either enthusiastic or skeptical) towards a certain treatment, based on solid evidence or convincing clinical experience is less likely to be affected by new evidence than one's neutral attitude towards a specific treatment. In Bayesian inference, prior beliefs are either informative (based on evidence or clinical experience) or non-/weakly informative. Such a weakly informative prior aims to yield posterior probabilities that are influenced almost exclusively by the actual trial data. Given the potential influence of informed priors on posterior probability distributions, it is essential to define priors realistically and before trial results become available (@kruschke2021bayesian).

Historically, clinical trials have been evaluated by the use of frequentist inference, by which the probability of the data is tested, assuming the null hypothesis (no difference). Such an approach heavily depends on the trial's power, which in turn is the result of the included sample size and the treatment effect. Clinical trials are often time- and resource-consuming, which has led investigators to base their sample size calculation on an (optimistic) expected treatment effect, rather than a clinically important treatment effect. When the null hypothesis is not rejected in these cases, this may be the consequence of a reduced power, and this might cause critical care physicians to abandon therapies that have a potentially clinically important benefit (@yarnell2021clinical). In contrast, the Bayesian frameworks allows the direct estimation of the posterior probability of any treatment effect, including the MCID. Finally, the incorporation of prior data may facilitate a more feasible sample size calculation, while the use of reference priors (such as enthusiastic and skeptical priors) can assess the robustness of the findings.

This secondary analysis will assess several outcomes and subgroups that were deemed most relevant to the overall study aim. The following outcomes will be assessed: **EQ-5D-5L health utility score** (longitudinal analysis), **6-min walking test and handgrip strength** over the entire follow-up period (longitudinal analyses), **60-day mortality**, **duration of mechanical ventilation** as well as **EQ-5D-5L health utility scores at 30, 90 and 180 days** (cross-sectional analyses). Based on the available literature, **patients with acute renal failure, sepsis and non-sepsis, and severe multi-organ failure at ICU admission were identified as relevant subgroups**. Acute renal failure is determined using the Kidney Disease: Improving Global Outcome (KDIGO) criteria for acute kidney injury (AKI) as stage I or higher. Sepsis is defined according to the Sepsis III criteria. Severe multi-organ failure is assessed using the Sequential Organ Failure Assessment (SOFA) score, for which we will use the median value of the SOFA score in our patient population to dichotomize patients with severe multi-organ failure (severe multiorgan failure will be defined as patients with $\geq$ median SOFA score). Finally, Non-surviving patients will be assigned an EQ-5D-5L health utility score of 0, in agreement with the trial protocol.

## Statistical Analysis

The Bayesian analyses will be performed using dedicated software, including `R` (@r2013r) and `JASP` (@gronau2019informed), which rely on the freely-available Bayesian software `JAGS` (@plummer2004jags) to implement the models under a Bayesian framework via *Markov Chain Monte Carlo* methods (@brooks1998markov). Baseline data will be presented in the primary trial publication as specified elsewhere. If prior data from previous randomized trials is available to formulate an informative (literature-based) prior, such a prior will be incorporated. When no prior trial data are available, analyses will be performed under a weakly informative prior. In addition, skeptical and enthusiastic priors will be used to assess the robustness of the results. In the following sections, the components of the Bayesian analyses will be outlined.

### Priors

For each endpoint, an MCID is derived from the literature (@tbl-endpoint).

| Outcome               | Effect size and Approach | Non-info (mean, sd) | Info (mean, sd) | MCID   |
|---------------|---------------|---------------|---------------|---------------|
| EQ-5D                 | MD, longitudinal         | (0,6)               |                 | 0.06   |
| 6MWT                  | MD, longitudinal         | (0,1900)            |                 | 19     |
| HGS                   | MD, longitudinal         | (0,500)             |                 | 5      |
| Duration of MV        | MD, cross-sectional      | (0,100)             | (-0.42, 0.30)   | 1      |
| 60-day mortality      | OR, cross-sectional      | (0, 3.0)            | (-0.02, 0.09)   | 5% ARD |
| EQ-5D (30 days)       | MD, cross-sectional      | (0,6)               |                 | 0.06   |
| EQ-5D (90 days)       | MD, cross-sectional      | (0,6)               |                 | 0.06   |
| EQ-5D (180 days)      | MD, cross-sectional      | (0,6)               |                 | 0.06   |
| EQ-5D (Sepsis yes/no) | MD, longitudinal         | (0,6)               |                 | 0.06   |
| EQ-5D (AKI yes/no)    | MD, longitudinal         | (0,6)               |                 | 0.06   |
| EQ-5D (Fail yes/no)   | MD, longitudinal         | (0,6)               |                 | 0.06   |

: Prior distributions information and MCIDs {#tbl-endpoint}

For all analyses, we will use weakly informative priors centered around ‘no effect’ (for example a mean difference MD of 0, or an odds ratio OR of 1 0 on the log OR scale). For the binary outcomes (ORs, denoted as the log of the OR), a mean of 0 will be applied for the weakly informative prior, while the standard deviation (SD) will be set to 3 on the log OR scale, to capture all credible effect sizes. For the continuous outcomes (on the MD scale), we aim to be consistent and reproducible, but will also allow the distributions to capture all plausible effect sizes. As such, the standard deviation (SD) will be based on a multiplication of the MCID (x100). @tbl-endpoint presents the numerical values of these weakly informative priors. Skeptical and enthusiastic priors are defined following a modification of the approach suggested by de Grooth and Elbers (@de2022pick). Skeptical priors will be centered at a mean difference (MD) or log OR of 0. The distribution will incorporate a $<10\%$ probability that the estimated treatment effect will exceed +1 MCID. Conversely, the enthusiastic priors are centered around an effect of +2 MCID, and will follow a similar distribution with a probability of $<10\%$ that estimated effect size will be lower than +1 MCID (@fig-priors).

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: fig-priors
#| out-width: '80%'

knitr::include_graphics('./skep_ent_priors.png')
```

For the cross-sectional endpoints “60-day mortality” and “duration of mechanical ventilation”, informative priors could be derived from a meta-analysis of randomized trials addressing the clinical effectiveness of high protein nutrition in critical illness, which has recently been updated by the same authors after the publication of the EFFORT Protein trial. Data from this updated meta-analysis that are relevant to the current Bayesian analysis protocol were kindly shared with us by the authors prior to publication. This meta-analysis also contains one study that reports on EQ-5D-5L, albeit on a survivors-only analysis. Since the PRECISe trial uses a complete-case analysis (including non-survivors), these data could not be used to formulate a reasonable literature based prior for the estimation of the treatment effect on this outcome. Therefore, cross-sectional and longitudinal analyses of EQ-5D-5L will be performed under weakly informative priors, skeptical priors, and enthusiastic priors.

Finally, as all analyses will be performed with adjustment for the random center effect, a prior for this effect is uniformly formulated as well. These models incorporate random intercepts and the prior for these random effects follow a normal distribution with an effect centered around a mean of 0 and a large standard deviation, similar to the other priors. Posterior distributions will be presented as MDs or mean ARDs and median OR, accompanied by $95\%$ credible intervals (CrI), and reference to the used priors. Furthermore, full posterior probability distributions will be presented in dedicated grid plots.

### Analysis methods and assumptions

The primary outcome is the EQ-5D-5L health utility score over the first 180 days following ICU admission. A pre-planned interim safety analysis revealed a bimodal distribution for EQ-5D-5L since non-survivors (39% during interim analysis) were attributed with a health utility score of zero. Given this mixture distribution (the component of zero, and the component other than 0) we will specify separate priors per longitudinally assessed outcome. Consequently, we will specify a prior for the mean difference with an EQ-5D-5L other than 0, and a prior for the proportion of patients who have an EQ-5D-5L score of 0 (i.e., deceased patients). This longitudinal analysis will be performed with adjustment for center as a random effect. The results of the analyses for the components will be presented separately and as weighted averages.

Secondary outcomes for which no prior evidence was available are the 6-min walking test and hand grip strength. As such, the posterior probabilities of these outcomes will be estimated under a weakly informative prior, in a model similar to the longitudinally assessed primary outcome, with adjustment for the random effect of center. Based on these probability distributions, the probability of clinically important benefit and harm will be estimated. Secondary binary outcomes, such as 60-day mortality, will be expressed in ORs and absolute risk differences (ARD). These binary outcomes will be analyzed in a binary mixed regression model (Bernoulli distribution) with an adjustment for the random center effect. Priors for these binary outcomes are specified on the log OR scale. Other secondary continuous outcomes, such as the duration of mechanical ventilation, will be reported in mean difference (MD) for the specific units of that endpoint. Also for these analyses, the posterior probabilities of a clinically meaningful benefit and clinically important harm will be estimated. Finally, the same mixture distribution (the component of zero, and the component other than 0) will be used for the EQ-5D-5L assessment at the cross-sectional time points, and separate priors will be formulated, similar to the primary outcome assessment.

As outcome missing values are assumed to fully depend on the observed data, and in agreement with the protocol for the frequentist analysis of our study, for all analyses we rely on a missing at random (MAR) assumption about the mechanism responsible for the occurrence of missing values (@little2019statistical). We rely on linear mixed effects models to analyse the complete (missing and observed) data as they are typically considered appropriate to handle missingness under MAR.

All Bayesian models for our analysis are implemented in `JAGS` using MCMC algorithms, and implemented via `R` through the `R2jags` package (@su2015package). Assessment of model convergence will be performed for key model parameters via potential scale reduction factors (Rhat) effective sample size (ESS), and other diagnostics such as density and trace plots. Model fit will be assessed in relative terms through the deviance information criterion (DIC and other criteria alike), and in absolute terms by use of posterior prediction checks (PPCs).

## Data Description

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false

#load packages needed for analysis
library(foreign)
library(data.table)
library(kableExtra)
library(bookdown)
library(reshape)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(plotrix)
library(emmeans)
library(nlme)
library(lme4)
library(lmerTest)
library(readxl)
library(tidyverse)
library(gt)
library(gtsummary)
library(broom.mixed)
library(mice)
library(miceadds)
library(mitml)
library(boot)
library(BCEA)

#remove scientific notation for numbers
options(scipen=999)

#set local path to import data
#setwd("C:/Users/andre/Documents/maastrict/consultancy/trials/precise/bayesian_analysis/analysis")

#load dataset (935 observations on 27 variables)
dataset <- read_excel("Database PRECISe for Andrea.xlsx", sheet = 1, col_names = TRUE, na = "NA")

#questions:
#1) in original frequentist analysis (https://github.com/sandervkuijk/PRECISe/blob/main/PRECISe_Ana_1_Prim_V1.0.R), the outcome variable EQ5D.HUS.imp0 and the baseline proxy utility variable EQ5D.HUS.proxy.0 are used in the model - not sure how these differ from those in the dataset provided by Sam 

#2) in sensitivity analysis explored in the frequentist approach (see above), the model with the additional predictors DEM_SEX, APACHEscore, ADM_SYSTEMDIAGNOSIS, ADM_TYPEOPT, and NRS score has been applied. However, I do not have access to most of these other variables; are they needed for our analysis?


#define elements/variables for summary statistics/analysis
#number of patients
N <- dim(dataset)[1]
#re-code id number of patients
id <- seq(1:N)
#use number of center and create factor variable
center <- dataset$`INSTITUTION CODE`
center_f <- factor(center)
#re-code trt indicator (0=control,1=intervention)
trt <- ifelse(dataset$`RANDOMIZATION GROUP`== "Group 1",0,1)
trt_f <- factor(trt)
#age (years)
age <- dataset$AGE
#re-code sex (0 = , 1 = )
sex <- dataset$SEX
sex_f <- factor(sex-1)
#weight (Kg)
weight <- dataset$WEIGT
#height (cm)
height <- dataset$HEIGHT
#BMI
bmi <- dataset$BMI
#diabetes (0=no, 1=yes)
diabete <- dataset$DIABETES
diabete_f <- factor(diabete)
#re-code admission type (0=,1=)
admission <- dataset$`ADMISSION TYPE`
admission_f <- factor(admission-1)
#covid (0=no, 1=yes)
covid <- dataset$COVID
covid_f <- factor(covid)
#AKI (0=no, 1=yes)
aki <- dataset$AKI
aki_f <- factor(aki)
#sepsis (0=no, 1=yes)
sepsis <- dataset$SEPSIS
sepsis_f <- factor(sepsis)
#organ failure (0=no, 1=yes)
failure <- dataset$`ORGAN FAILURE`
failure_f <- factor(failure)
#qol baseline
qol_base <- dataset$`QoL 0 days (proxy)`
#qol 30 days
qol_1 <- dataset$`QoL 30 days`
#qol 90 days
qol_2 <- dataset$`QoL 90 days`
#qol 180 days
qol_3 <- dataset$`QoL 180 days`
#handgrip 30 days
hand_1 <- dataset$`HANDGRIP 30`
#handgrip 90 days
hand_2 <- dataset$`HANDGRIP 90`
#handgrip 180 days
hand_3 <- dataset$`HANDGRIP 180`
#smwt 30 days
smwt_1 <- dataset$`SMWT 30`
#smwt 90 days
smwt_2 <- dataset$`SMWT 90`
#smwt 180 days
smwt_3 <- dataset$`SMWT 180`
#mortality 60 days (0=alive,1=dead)
mort60 <- dataset$`MORTALITY 60 days`
mort60_f <- factor(mort60)
#mv dur (days)
mv_dur <- dataset$`MV duration (days)`

#create new dataset with updated variables to create summary statistics
data_descr <- data.frame(id,center_f,center,trt,trt_f,age,sex,sex_f,weight,height,bmi,diabete,diabete_f,admission,admission_f,covid,covid_f,aki,aki_f,sepsis,sepsis_f,failure,failure_f,qol_base,qol_1,qol_2,qol_3,hand_1,hand_2,hand_3,smwt_1,smwt_2,smwt_3,mort60,mort60_f,mv_dur)
#data_descr <- data.table(data_descr)

#summary statistics for baseline variables across arms
gt_sum_baseline <-
  tbl_summary(
    data_descr,
    type = list(trt ~ "categorical", center_f ~ "categorical", sex_f ~ "categorical", 
                diabete_f ~ "categorical", admission_f ~ "categorical", covid_f ~ "categorical",
                aki_f ~ "categorical",sepsis_f ~ "categorical",failure_f ~ "categorical",
                weight ~ "continuous", height ~ "continuous", bmi ~ "continuous", qol_base ~ "continuous"
                ),
    include = c(trt, center_f, sex_f, diabete_f, admission_f, covid_f, aki_f, sepsis_f, failure_f, weight, height, bmi, qol_base),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    ),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_baseline <- as_gt(gt_sum_baseline) 

#summary statistics for baseline variables by arm

gt_sum_baseline_arm <-
  tbl_summary(by = trt,
    data_descr,
    type = list(center_f ~ "categorical", sex_f ~ "categorical", 
                diabete_f ~ "categorical", admission_f ~ "categorical", covid_f ~ "categorical",
                aki_f ~ "categorical",sepsis_f ~ "categorical",failure_f ~ "categorical",
                weight ~ "continuous", height ~ "continuous", bmi ~ "continuous", qol_base ~ "continuous"),
    include = c(center_f, sex_f, diabete_f, admission_f, covid_f, aki_f, sepsis_f, failure_f, weight, height, bmi, qol_base),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    ),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_baseline_arm <- as_gt(gt_sum_baseline_arm) 


#summary statistics for key follo-up outcome variables across arms
gt_sum_fu <-
  tbl_summary(
    data_descr,
    include = c(qol_1, qol_2, qol_3, hand_1, hand_2, hand_3, smwt_1, smwt_2, smwt_3, mort60_f, mv_dur),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    ),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu <- as_gt(gt_sum_fu) 

#summary statistics for baseline variables by arm

gt_sum_fu_arm <-
  tbl_summary(by = trt,
    data_descr,
    include = c(qol_1, qol_2, qol_3, hand_1, hand_2, hand_3, smwt_1, smwt_2, smwt_3, mort60_f, mv_dur),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    ),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_arm <- as_gt(gt_sum_fu_arm) 

##############################
#subgroup analyses

#summary statistics for outcome variables at follow-up across arms by AKI status

gt_sum_fu_aki <-
  tbl_summary(by = aki,
    data_descr,
#    type = list(qol_1 ~ "continuous", qol_2 ~ #"continuous", qol_3 ~ "continuous"),
    include = c(qol_1, qol_2, qol_3),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_aki <- as_gt(gt_sum_fu_aki) 

#summary statistics for outcome variables at follow-up by arms and by AKI status
data_descr_aki0 <- data_descr[data_descr$aki==0,]
data_descr_aki1 <- data_descr[data_descr$aki==1,]

gt_sum_fu_aki0_trt <-
  tbl_summary(by = trt,
    data_descr_aki0,
#    type = list(qol_1 ~ "continuous", qol_2 ~ #"continuous", qol_3 ~ "continuous"),
    include = c(qol_1, qol_2, qol_3),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_aki0_trt <- as_gt(gt_sum_fu_aki0_trt) 

gt_sum_fu_aki1_trt <-
  tbl_summary(by = trt,
    data_descr_aki1,
#    type = list(qol_1 ~ "continuous", qol_2 ~ #"continuous", qol_3 ~ "continuous"),
    include = c(qol_1, qol_2, qol_3),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_aki1_trt <- as_gt(gt_sum_fu_aki1_trt) 

gt_sum_arm_gt_aki <- tbl_stack(list(gt_sum_fu_aki0_trt, gt_sum_fu_aki1_trt), group_header = c("trt=0", "trt=1"))
gt_sum_arm_gt_aki_final <- as_gt(gt_sum_arm_gt_aki) 


#summary statistics for outcome variables at follow-up across arms by sepsis status

gt_sum_fu_sepsis <-
  tbl_summary(by = sepsis,
    data_descr,
#    type = list(qol_1 ~ "continuous", qol_2 ~ #"continuous", qol_3 ~ "continuous"),
    include = c(qol_1, qol_2, qol_3),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_sepsis <- as_gt(gt_sum_fu_sepsis) 

#summary statistics for outcome variables at follow-up by arms and by sepsis status
data_descr_sepsis0 <- data_descr[data_descr$sepsis==0,]
data_descr_sepsis1 <- data_descr[data_descr$sepsis==1,]

gt_sum_fu_sepsis0_trt <-
  tbl_summary(by = trt,
    data_descr_sepsis0,
#    type = list(qol_1 ~ "continuous", qol_2 ~ #"continuous", qol_3 ~ "continuous"),
    include = c(qol_1, qol_2, qol_3),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_sepsis0_trt <- as_gt(gt_sum_fu_sepsis0_trt) 

gt_sum_fu_sepsis1_trt <-
  tbl_summary(by = trt,
    data_descr_sepsis1,
#    type = list(qol_1 ~ "continuous", qol_2 ~ #"continuous", qol_3 ~ "continuous"),
    include = c(qol_1, qol_2, qol_3),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_sepsis1_trt <- as_gt(gt_sum_fu_sepsis1_trt) 

gt_sum_arm_gt_sepsis <- tbl_stack(list(gt_sum_fu_sepsis0_trt, gt_sum_fu_sepsis1_trt), group_header = c("trt=0", "trt=1"))
gt_sum_arm_gt_sepsis_final <- as_gt(gt_sum_arm_gt_sepsis) 


#summary statistics for outcome variables at follow-up across arms by multi organ failure status

gt_sum_fu_failure <-
  tbl_summary(by = failure,
    data_descr,
#    type = list(qol_1 ~ "continuous", qol_2 ~ #"continuous", qol_3 ~ "continuous"),
    include = c(qol_1, qol_2, qol_3),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_failure <- as_gt(gt_sum_fu_failure) 

#summary statistics for outcome variables at follow-up by arms and by failure status
data_descr_failure0 <- data_descr[data_descr$failure==0,]
data_descr_failure1 <- data_descr[data_descr$failure==1,]

gt_sum_fu_failure0_trt <-
  tbl_summary(by = trt,
    data_descr_failure0,
#    type = list(qol_1 ~ "continuous", qol_2 ~ #"continuous", qol_3 ~ "continuous"),
    include = c(qol_1, qol_2, qol_3),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_failure0_trt <- as_gt(gt_sum_fu_failure0_trt) 

gt_sum_fu_failure1_trt <-
  tbl_summary(by = trt,
    data_descr_failure1,
#    type = list(qol_1 ~ "continuous", qol_2 ~ #"continuous", qol_3 ~ "continuous"),
    include = c(qol_1, qol_2, qol_3),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"),
    digits = all_continuous() ~ 2,
    missing = "always" # don't list missing data separately
  ) %>%
  add_n(statistic = "{N_obs} {p_nonmiss}%", col_label = "**N** **obs%**") %>% # add column with total number of non-missing observations
  modify_header(label = "**Variable**") %>% # update the column header
  bold_labels() 

gt_sum_gt_fu_failure1_trt <- as_gt(gt_sum_fu_failure1_trt) 

gt_sum_arm_gt_failure <- tbl_stack(list(gt_sum_fu_failure0_trt, gt_sum_fu_failure1_trt), group_header = c("trt=0", "trt=1"))
gt_sum_arm_gt_failure_final <- as_gt(gt_sum_arm_gt_failure) 

```

@tbl-sum1 and @tbl-sum2 report summary statistics, either across or by treatment arm (0=standard,1=intervention), for key variables collected in the study at baseline. These include: **center, sex (f/m), age (years), diabetes (y/n), admission type, covid (y/n), AKI (y/n), sepsis (y/n), multiple organ failure (y/n), weight (Kg), height (cm), BMI, EQ-5D utility score**. The summary statistics displayed for each baseline variable include: number of unobserved values (unknown), mean (sd) for continuous variables, and number of observed values ($\%$) for categorical variables.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum1
#| tbl-cap: Summary statistics of key baseline variables in the study across arms.

gt_sum_gt_baseline

#gt_sum_gt_baseline %>%
#  tab_caption(caption = md("Summary statistics of key baseline variables in the study across arms."))
```

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum2
#| tbl-cap: Summary statistics of key baseline variables in the study by treatment arm (0=standard,1=intervention).

gt_sum_gt_baseline_arm

#gt_sum_gt_baseline_arm %>%
#tab_caption(caption = md("Summary statistics of key baseline variables in the study by #treatment arm (0=standard,1=intervention)."))
```

@tbl-sum3 and @tbl-sum4 report summary statistics, either across or by treatment arm (0=standard,1=intervention), for key outcome variables collected in the study after baseline. These include: **EQ-5D utility score at 30, 90, 180 days, six-min walking test at 30, 90, 180 days, handgrip strength at 30, 90, 180 days, mortaility at 60 days, duration of mechanical ventilation**. The summary statistics displayed for each outcome variable include: number of unobserved values (unknown), mean (sd) for continuous variables, and number of observed values ($\%$) for categorical variables.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum3
#| tbl-cap: Summary statistics of key follo-up outcome variables in the study across arms.

gt_sum_gt_fu

#gt_sum_gt_fu %>%
#tab_caption(caption = md("Summary statistics of key follo-up outcome variables in the study #across arms."))
```

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum4
#| tbl-cap: Summary statistics of key follow-up outcome variables in the study by treatment arm (0=standard,1=intervention).

gt_sum_gt_fu_arm

#gt_sum_gt_fu_arm %>%
#tab_caption(caption = md("Summary statistics of key follow-up outcome variables in the #study by treatment arm (0=standard,1=intervention)."))
```

### Descriptives by subgroups

@tbl-sum5 and @tbl-sum6 report summary statistics, either across or by treatment arm (0=standard,1=intervention), for EQ-5D utility score variables collected in the study after baseline separated for different subgroups based on AKI status (0=no,1=yes).

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum5
#| tbl-cap: Summary statistics of EQ-5D utility score follow-up outcome variables in the study by AKI status (0=no,1=yes).

gt_sum_gt_fu_aki

#gt_sum_gt_fu_aki %>%
#tab_caption(caption = md("Summary statistics of EQ-5D utility score follow-up outcome #variables in the study by AKI status (0=no,1=yes)."))
```

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum6
#| tbl-cap: Summary statistics of EQ-5D utility score follow-up outcome variables in the study by treatment arm (0=standard,1=intervention) and AKI status (0=no,1=yes).

gt_sum_arm_gt_aki_final

#gt_sum_arm_gt_aki_final %>%
#tab_caption(caption = md("Summary statistics of EQ-5D utility score follow-up outcome #variables in the study by treatment arm (0=standard,1=intervention) and AKI status #(0=no,1=yes)."))
```

@tbl-sum7 and @tbl-sum8 report summary statistics, either across or by treatment arm (0=standard,1=intervention), for EQ-5D utility score variables collected in the study after baseline separated for different subgroups based on sepsis status (0=no,1=yes).

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum7
#| tbl-cap: Summary statistics of EQ-5D utility score follow-up outcome variables in the study by treatment arm (0=standard,1=intervention) and AKI status (0=no,1=yes).

gt_sum_gt_fu_sepsis

#gt_sum_gt_fu_sepsis %>%
#tab_caption(caption = md("Summary statistics of EQ-5D utility score follow-up outcome #variables in the study by sepsis status (0=no,1=yes)."))
```

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum8
#| tbl-cap: Summary statistics of EQ-5D utility score follow-up outcome variables in the study by treatment arm (0=standard,1=intervention) and sepsis status (0=no,1=yes).

gt_sum_arm_gt_sepsis_final

#gt_sum_arm_gt_sepsis_final %>%
#tab_caption(caption = md("Summary statistics of EQ-5D utility score follow-up outcome #variables in the study by treatment arm (0=standard,1=intervention) and sepsis status #(0=no,1=yes)."))
```

@tbl-sum9 and @tbl-sum10 report summary statistics, either across or by treatment arm (0=standard,1=intervention), for EQ-5D utility score variables collected in the study after baseline separated for different subgroups based on multi organ failure status (0=no,1=yes).

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum9
#| tbl-cap: Summary statistics of EQ-5D utility score follow-up outcome variables in the study by multi organ failure status (0=no,1=yes).

gt_sum_gt_fu_failure

#gt_sum_gt_fu_failure %>%
#tab_caption(caption = md("Summary statistics of EQ-5D utility score follow-up outcome #variables in the study by multi organ failure status (0=no,1=yes)."))
```

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-sum10
#| tbl-cap: Summary statistics of EQ-5D utility score follow-up outcome variables in the study by treatment arm (0=standard,1=intervention) and multi organ failure status (0=no,1=yes).

gt_sum_arm_gt_failure_final

#gt_sum_arm_gt_failure_final %>%
#tab_caption(caption = md("Summary statistics of EQ-5D utility score follow-up outcome #variables in the study by treatment arm (0=standard,1=intervention) and multi organ failure #status (0=no,1=yes)."))
```

## Analysis of primary outcome - EQ-5D utility score at multiple times

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false

#eq5d analysis (30, 90, 180 days)
#check zero values at each time
data_descr$zero_d1 <- ifelse(data_descr$qol_1==0,1,0)
data_descr$zero_d2 <- ifelse(data_descr$qol_2==0,1,0)
data_descr$zero_d3 <- ifelse(data_descr$qol_3==0,1,0)
data_descr$zero_d <- ifelse(data_descr$zero_d1==1 & data_descr$zero_d2==1 & data_descr$zero_d3==1,1,0)
#get completed dataset
data_descr_cc <- data_descr[is.na(data_descr$qol_base)==FALSE,]
data_descr_cc$id_cc <- seq(1:length(data_descr_cc$id))

#convert dataset to long format to perform mixed models
dataset_long_eq5d <- reshape(data_descr, varying = c("qol_1","qol_2","qol_3"), direction = "long", idvar = "id", sep = "_")
dataset_long_eq5d$time_f <- factor(dataset_long_eq5d$time)
#convert completed dataset
dataset_long_eq5d_cc <- reshape(data_descr_cc, varying = c("qol_1","qol_2","qol_3"), direction = "long", idvar = "id", sep = "_")
dataset_long_eq5d_cc$time_f <- factor(dataset_long_eq5d_cc$time)

#check number/prop zeros for utility scores
nzero_qol_1 <- sum(data_descr$zero_d[data_descr$zero_d1==1]==1, na.rm=T)
pzero_qol_1 <- sum(data_descr$zero_d[data_descr$zero_d1==1]==1, na.rm=T)/length(data_descr$qol_1[complete.cases(data_descr$qol_1)])

nzero_qol_2 <- sum(data_descr$zero_d[data_descr$zero_d2==1]==1, na.rm=T)
pzero_qol_2 <- sum(data_descr$zero_d[data_descr$zero_d2==1]==1, na.rm=T)/length(data_descr$qol_2[complete.cases(data_descr$qol_2)])

nzero_qol_3 <- sum(data_descr$zero_d[data_descr$zero_d3==1]==1, na.rm=T)
pzero_qol_3 <- sum(data_descr$zero_d[data_descr$zero_d3==1]==1, na.rm=T)/length(data_descr$qol_3[complete.cases(data_descr$qol_3)])
```

In this section the results from a longitudinal analysis conducted on the EQ-5D utility scores measured at $t_1=30$, $t_2=90$ and $t_3=180$ days are reported. For this analysis, a linear mixed-effects regression model is used to estimate the average difference in utility score between treatment arms over the study period, after controlling baseline utility values and the clustering effects at centres level. A potential issues when implementing this modelling strategy is the typical asymmetry of EQ-5D utility score data, which in the current study is particularly exacerbated by the considerable number of zero values observed at any time point. More specifically, the number (proportions) of observed individuals associated with a zero utility score at each follow-up time across arms are: $`r nzero_qol_1` (`r round(pzero_qol_1,2)*100`\%)$ at 30 days, $`r nzero_qol_2` (`r round(pzero_qol_2,2)*100`\%)$ at 90 days, $`r nzero_qol_3` (`r round(pzero_qol_3,2)*100`\%)$ at 180 days. @fig-histqol shows histograms of the observed distributions of EQ-5D utility scores at each follow-up point in the study.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: fig-histqol
#| fig-cap: "Histograms of the empirical distributions of EQ-5D utility score at each time point in the study."

dataset_long_eq5d$time_days <- dataset_long_eq5d$time
dataset_long_eq5d$time_days <- ifelse(dataset_long_eq5d$time==1,"30",dataset_long_eq5d$time_days)
dataset_long_eq5d$time_days <- ifelse(dataset_long_eq5d$time==2,"90",dataset_long_eq5d$time_days)
dataset_long_eq5d$time_days <- ifelse(dataset_long_eq5d$time==3,"180",dataset_long_eq5d$time_days)
dataset_long_eq5d$time_days <- factor(dataset_long_eq5d$time_days, levels = c("30","90","180"))

# plot
ggplot(dataset_long_eq5d, aes(x=qol)) +
    geom_histogram(binwidth = 0.1) +
    facet_wrap(~time_days) + 
    xlab("EQ-5D utility") + ylab("count") +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 400)) +
    theme(axis.title = element_text(face="bold"), panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), panel.background = element_blank(), 
        axis.line = element_line(colour = "black"), plot.title = element_text(hjust = 0.5),
        legend.position = "none")

```

To address the presence of excess zeros in the data, we modify the standard modelling structure and fit the mixed-regression framework only to the observed utility values that are different from zero and that remain so across all times, while implementing a logistic regression to estimate the proportion of these zero values. Such modelling strategy is known as **two-part regression** or **hurdle** approach (@farewell2017two), consisting in modelling the data through a finite mixture distribution formed by two components: one component is associated with the values that occur multiple times (e.g. zero) which are considered as constant with no uncertainty; a second component formed by all other values that are modelling via some appropriate probability distribution (e.g. Normal distribution). An overall estimate for the treatment effect of interest, e.g. mean difference, can then be simply retrieved as a weighted linear combination between the estimated derived from the non-zero component and zero with weights given by the corresponding estimated probabilities of observing a non-zero and zero value from the data.

When implemented to our utility data, the hurdle model is specified as:

-   A standard mixed-regression model fitted to all utility values different from zero ($u_j \neq 0$, for $j=1,2,3$)

$$
u^{\neq 0}_{ij} = \beta_0 + \beta_1 \times \text{trt}_i + \beta_2\times \text{time}^\star + \beta_3 \times \text{trt}_i\times \text{time}^\star + \beta_4 \times u_{i0}^\star + \omega_{j} +\varepsilon_{ij},
$$ 

where $u_{ij}^{\neq 0}$ denotes the utility score different from zero for patient $i$ at time $j$, $\text{trt}_i$ the treatment arm indicator, $\text{time}^\star$ the centred time indicator, $u_{i0}^\star$ the centred baseline utility score, while $\omega_{j}$ and $\varepsilon_{ij}$ the centre-specific random error term and the residual term, respectively associated with variance parameters $\sigma^2_{\omega}$ and $\sigma^2_{\varepsilon}$. Adjusted estimates of mean differences in utility between arms over time ($\Delta^{\neq 0}_u$) can be simply retrieved as the regression coefficient $\beta_1$.

-   A logistic regression model fitted to a binary indicator variable denoting which patients are associated with a zero utility value ($d_{i}=1$ if $u_{ij}=0$ at all $j$)

$$
\text{logit}(d_{i}) = \gamma_0 + \gamma_1 \times \text{trt}_i + \gamma_2 \times u_{i0}^\star,
$$

where $d_{i}:=\mathbb{I}(u_{i}=0)$ is the zero-indicator for the utility of patient $i$. Estimates of adjusted odds ratios of being associated with a zero value between treatment arms can be retrieved from taking the exponential of the regression coefficient $\gamma_1$ or, equivalently, estimates of the probability of having a zero value for each treatment can be obtained as simple linear combination of the regression coefficients from the model, i.e. $\pi^0_0 = P(u_{i}=0 \mid \text{trt}=0)=e^{\gamma0}$ and $\pi^0_1 = P(u_{i}=0 \mid \text{trt}=1)=e^{\gamma0+\gamma_1}$.

Estimates for the overall effect expressed in terms of mean differences in utility scores between treatment arms can then be retrieved as the weighted average:

$$
\Delta_{u} = \Delta^{\neq 0}_u \times (1 - (\pi^0_1 - \pi^0_0)),
$$

that is as the product between the mean difference between arms in non-zero utility score and the corresponding difference in probabilities between arms of observing a non-zero value. Thanks to the flexibility of the Bayesian framework, the extension of the standard mixed-regression modelling framework can be implemented in a relatively easy way and full posterior distributions can be derived for any unobserved quantity of interest from the analysis.

### Priors

Priors on each parameters are elicited in accordance to the different types of analyses pre-specified in the protocol of this Bayesian analysis (@heuts2024impact), which are also summarised in @tbl-endpoint. The base-case analysis is implemented under weakly-informative priors for key parameters while, when available, an informative prior specification is additionally specified based on available evidence retrieved from the literature about the parameters of interest.

Finally, two sensitivity analyses to prior specification are also conducted using either skeptical or enthusiastic priors, following the guidelines and principles outlined by @de2022pick.

For example, in the case of the primary outcome, taking as reference the prior specified on the regression parameters representing the mean difference in EQ-5D utility score ($\beta_1$) or the log odds ratio in terms of chance of observing a zero outcome value between the arms ($\gamma_1$), the following priors are specified under each type of analysis:

1.  Weakly-informative:

$$
\beta_1 \sim \text{Normal}(0,6); \;\;\;\ \gamma_1 \sim \text{Normal}(0,3);
$$

2.  Literature-based: not available

3.  Skeptical:

$$
\beta_1 \sim \text{Normal}(0,0.047); \;\;\;\ \gamma_1 \sim \text{Normal}(0,3);
$$

4.  Enthusiastic:

$$
\beta_1 \sim \text{Normal}(0.12,0.047); \;\;\;\ \gamma_1 \sim \text{Normal}(0,3);
$$

Note that for the skeptical and enthusiastic prior specifications, hyper prior values are chosen such that the respective prior probabilities $P(\beta_1>\text{MCID})=0.1$ and $P(\beta_1 < \text{MCID})=0.1$ are satisfied.

### Results from base-case analysis (weakly informative priors)

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| cache-lazy: false

#pre-process data
#remove all cases with missing predictors (could impute via distribution but would require further prior specifications not said in the pre-planned analysis)
#dataset_long_eq5d <- dataset_long_eq5d[is.na(dataset_long_eq5d$qol_base)==FALSE,]
dataset_long_eq5d <- dataset_long_eq5d_cc
#dataset_long_eq5d$timem <- scale(dataset_long_eq5d$time, center = TRUE, scale = FALSE)
#try frequentist
#library(nlme)
#fit_1 <- lme(qol ~ trt*timem + qol_base, data = dataset_long_eq5d,
#             na.action = na.omit,
#             random = list(center = ~ 1,
#                           id = ~ 1))
#summary(fit_1)



#define each variable to be passed to JAGS
id <- dataset_long_eq5d$id_cc
qol <- dataset_long_eq5d$qol
trt <- dataset_long_eq5d$trt
time <- as.vector(scale(dataset_long_eq5d$time, center = TRUE, scale = FALSE))
qol_base <- as.vector(scale(dataset_long_eq5d$qol_base, center = TRUE, scale = FALSE)) 
zero_d <- dataset_long_eq5d$zero_d
Nt <- length(dataset_long_eq5d$id)
N <- length(unique(dataset_long_eq5d$id))
P <- 4 #number of predictors in the qol model
L <- 10 #number of centres
center <- dataset_long_eq5d$center

#hyper prior values
prior_md_ms <- c(0,6) #means and sd for mean difference
prior_md_mp <- c(0,1/6^2) #means and precision for mean difference
prior_lor_ms <- c(0,3) #means and sd for log OR
prior_lor_mp <- c(0,1/3^2) #means and precision for log OR

#build model in JAGS
model_mix_qol <- "
model{
for(i in 1:Nt){
 #qol model - non-zero
      qol[i] ~ dnorm(mu.u[i], tau.u[zero_d[i]+1])
      mu.u[i] <- a0[zero_d[i]+1,id[i]] + b0[zero_d[i]+1,center[i]] + beta0[zero_d[i]+1] + beta[1,zero_d[i]+1]*trt[i] + beta[2,zero_d[i]+1]*time[i] + beta[3,zero_d[i]+1]*trt[i]*time[i] + beta[4,zero_d[i]+1]*qol_base[i]
 #eq5d model - zero
      zero_d[i] ~ dbern(pi[i])
      logit(pi[i]) <- gamma0 + gamma[1]*trt[i] + gamma[2]*qol_base[i]
  }
 #center random effects
  for(l in 1:L){
      b0[1,l] ~ dnorm(0, tau.b0[1])
      b0[2,l] ~ dnorm(0, tau.b0[2])
 }
 #person within center random effects 
 for(n in 1:N){
      a0[1,n] ~ dnorm(0, tau.a0[1])
      a0[2,n] ~ dnorm(0, tau.a0[2])
 }
 
 #convert precision to sd
 for(d in 1:2){
 tau.u[d] <- 1/var.u[d]
 var.u[d] <- sd.u[d]*sd.u[d]
 tau.a0[d] <- 1/var.a0[d]
 var.a0[d] <- sd.a0[d]*sd.a0[d] 
 tau.b0[d] <- 1/var.b0[d]
 var.b0[d] <- sd.b0[d]*sd.b0[d]
 }
 
 #priors on non-zero
 sd.u[1] ~ dunif(0,10)
 sd.a0[1] ~ dunif(0,10)
 sd.b0[1] ~ dunif(0,10)
 beta0[1] ~ dnorm(0, 0.001)
 beta[1,1] ~ dnorm(prior_md_mp[1],prior_md_mp[2])
  for(p in 2:P){
  beta[p,1] ~ dnorm(0,0.001)
  }

 #priors on zero
 sd.a0[2] ~ dunif(0,0.0000001)
 sd.b0[2] ~ dunif(0,0.0000001)
 beta0[2] <- 0
 sd.u[2] <- 0.0000001
  for(p in 1:P){
  beta[p,2] <- 0
  }
  
 #priors on logistic model
 gamma0 ~ dnorm(0,1)
 gamma[1] ~ dnorm(prior_lor_mp[1],prior_lor_mp[2])
 gamma[2] ~ dnorm(0,1)

 #save log likelihood
 for(i in 1:Nt){
 log_lik_qol[i] <- logdensity.norm(qol[i], mu.u[i], tau.u[zero_d[i]+1])
 log_lik_zero_d[i] <- logdensity.bern(zero_d[i], pi[i])
 }
 
 #obtain replications for pcc
 for(i in 1:Nt){
 qol_rep[i] ~ dnorm(mu.u[i], tau.u[zero_d[i]+1])
 zero_d_rep[i] ~ dbern(pi[i])
 }
 
 #save parameters of interest
 coef[1] <- beta0[1]
 coef[2] <- beta[1,1]
 coef[3] <- beta[2,1]
 coef[4] <- beta[3,1]
 coef[5] <- beta[4,1]
 coef[6] <- gamma0[1]
 coef[7] <- gamma[1]
 coef[8] <- gamma[2]
 
}

"

#save model file as txt
writeLines(model_mix_qol, "model.mixture.qol.txt")


#fit model
library(R2jags)
set.seed(3456)
datalist<-list("N","P","L","Nt","qol","qol_base","trt","time","id","zero_d","center","prior_lor_mp","prior_md_mp") #list of data input
inits <- function(){list()} #set default random initial values for all parameters
params<-c("beta0","beta","sd.a0","sd.b0","mu.u","sd.u","pi","gamma0","gamma","qol_rep","zero_d_rep","zero_d","log_lik_qol","log_lik_zero_d","coef") #list of parameters to save
filein <- "model.mixture.qol.txt" #name of file with model
n.iter <- 10000 #number of iterations
n.chains <- 2 #number of chains
n.thin <- 1 #thinning interval

model_mixture_qol <- jags(data=datalist,inits=inits,parameters.to.save=params,model.file=filein,n.chains=n.chains,n.iter=n.iter,n.thin=n.thin,DIC=TRUE)

#load function to extract summary results for selected parameters
source("jagsresults.R")

model_mixture_qol_sum <- round(jagsresults(x = model_mixture_qol, params = c('mu.u','pi', 'zero_d_rep', 'qol_rep', 'zero_d', 'trt', 'log_lik_qol', 'log_lik_zero_d','deviance','coef'), invert = TRUE), digits = 3)

#look at summary results
print(model_mixture_qol_sum)

#diagnostics
#check model convergence
library(ggmcmc)
library(mcmcr)
#extract posterior samples for all model parameters and convert them into mcmc object for inspection
jagsModel2_mcmcobject <- coda::as.mcmc(model_mixture_qol)
params_sel <- c("coef","sd.a0","sd.b0","sd.u")
jagsModel2_mcmcsubset <- subset(jagsModel2_mcmcobject, pars = params_sel)
jagsModel2_ggmcmc_object <- ggmcmc::ggs(jagsModel2_mcmcsubset)
```

For all quantities of interest, convergence of the model does not appear to be a concern. This is indicated by values of the *potential scale reduction factor statistics* (R hat) being below pretty close to $1$ for every parameter, as well as values of the *effective sample sizes* being not too small and in many cases close to the total number of MCMC iterations ($`r n.iter`$). We note that, for some parameters, the occurrence of estimates of $0$ with no uncertainty do not represent an issue in that they were intentionally set to those values to denote the specification of the zero-component mixture within the hurdle approach as implemented in `JAGS`.

Next, we examine additional diagnostics to check for potential issues in model convergence or fit to the observed data. This can be achieved, for example, by looking at visual diagnostics such as *posterior density* and *trace plots*.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: fig-diagnostics1
#| fig-cap: "Density plots to assess potential issues in convergence for the estimated mean difference in EQ-5D utility score between groups."

#density plots for key parameters
ggmcmc::ggs_density(jagsModel2_ggmcmc_object, family = "coef")+theme_classic()

```

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: fig-diagnostics2
#| fig-cap: "Trace plots to assess potential issues in convergence for the estimated mean difference in EQ-5D utility score between groups."

#diagnostics
#trace plots for key parameters
ggmcmc::ggs_traceplot(jagsModel2_ggmcmc_object, family = "coef")+theme_classic()
```

All visual diagnostics in @fig-diagnostics1 and @fig-diagnostics2 do not show any clear evidence of issues related to model convergence with relatively well-behaved densities and a good mixing of the MCMC chains for the parameter of interest. Next, we proceed to check model fit in absolute terms via *Posterior Predictive Checks* (PPCs), whose purpose is to check the plausibility of the predictions generated from the model with respect to the observed data. Although different ways to perform PPCs exist, we decided to rely on a visual comparison of model-based replicated outcome data and the original observed outcome data. The lack of deviations of the replicated data from the observed data provides some reassurance about the interpretability of the results from the model.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: fig-ppcs
#| fig-cap: "Posterior predictive checks based on the visual comparison of histograms of the original observed and model-based replicated EQ-5D utility score data."

#PPC
#extract model replications
qol_rep <- model_mixture_qol$BUGSoutput$sims.list$qol_rep
zero_d_rep <- model_mixture_qol$BUGSoutput$sims.list$zero_d_rep
#remove all missing values from original data
qol_cc <- qol[complete.cases(qol)]
zero_d_cc <- zero_d[complete.cases(zero_d)]
#remove imputed replicated data
qol_rep_cc <- matrix(NA,n.iter,length(qol_cc))
zero_d_rep_cc <- matrix(NA,n.iter,length(zero_d_cc))
for(i in 1:n.iter){
qol_rep_cc[i,] <- qol_rep[i,complete.cases(qol)]
zero_d_rep_cc[i,] <- zero_d_rep[i,complete.cases(zero_d)]
}

#compare model fit via relative information criteria (only if multiple models are run and need to be compared to chosse the best model - not applicable here)
#library(loo)
#log_lik_qol <- model_mixture_qol$BUGSoutput$sims.list$log_lik_qol
#ic_waic_qol <- waic(log_lik_qol)
#ic_looic_qol <- loo(log_lik_qol)

#check model fit
library(bayesplot)

#histogram between observed and (first 25) replicated outcome data
ppc_hist(qol_cc,qol_rep_cc[1:15,])
#additional plots
#histogram between observed and (first 25) replicated indicator data
#ppc_hist(zero_d_cc,zero_d_rep_cc[1:15,])

```

Once model performance and fit have been assessed and no major issues in the model is identified, as suggested by @fig-ppcs, we can proceed to inspect the posterior results for the parameters of interest. More specifically, for the primary outcome, we are interested in the posterior distribution of the mean difference in EQ-5D utility score between the treatment arms over time.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false

#obtain all quantities of interest from the posterior estimates of the model via Monte Carlo integration

#compute marginal means qol for each trt and mean difference
mu.u <- model_mixture_qol$BUGSoutput$sims.list$mu.u
sd.u <- model_mixture_qol$BUGSoutput$sims.list$sd.u
zero_d <- model_mixture_qol$BUGSoutput$sims.list$zero_d
zero0_d <- zero_d[,trt==0]
zero1_d <- zero_d[,trt==1]
mu0.u <- mu.u[,trt==0]
mu1.u <- mu.u[,trt==1]
mu0_zero_d_ind <- vector("list", n.iter)
mu1_zero_d_ind <- vector("list", n.iter)
for(i in 1:n.iter){
  mu0_zero_d_ind[[i]] <- which(zero0_d[i,]==0)
  mu1_zero_d_ind[[i]] <- which(zero1_d[i,]==0)
}
pi0.zero <- model_mixture_qol$BUGSoutput$sims.list$pi[, trt==0]
pi1.zero <- model_mixture_qol$BUGSoutput$sims.list$pi[, trt==1]
K <- 10000 #n replications to be used for MC integration
mm0.u.nonzero <- mm1.u.nonzero <- c()
p0.zero <- p1.zero <- c()
#generate many samples and take expectation across replications at each MCMC iteration to approximate marginal parameter values 
set.seed(3456)
for(i in 1:n.iter){
  mm0.u.nonzero[i] <- mean(rnorm(K, mean = mu0.u[i, mu0_zero_d_ind[[i]]], sd = sd.u[i, 1]))
  mm1.u.nonzero[i] <- mean(rnorm(K, mean = mu1.u[i, mu1_zero_d_ind[[i]]], sd = sd.u[i, 1]))
  p0.zero[i] <- mean(rbinom(K, size = 1, prob = pi0.zero[i, ]))
  p1.zero[i] <- mean(rbinom(K, size = 1, prob = pi1.zero[i, ]))
}
#compute all other quantities desired
MD.u.nonzero <-  mm1.u.nonzero - mm0.u.nonzero
ARD.zero <-  p1.zero - p0.zero
mm0.u.weighted <- mm0.u.nonzero*(1-p0.zero)
mm1.u.weighted <- mm1.u.nonzero*(1-p1.zero)
MD.u.weighted <- mm1.u.weighted - mm0.u.weighted
ODDS0.zero <- p0.zero/(1-p0.zero)
ODDS1.zero <- p1.zero/(1-p1.zero)
OR.zero <- ODDS1.zero/ODDS0.zero

#obtain summary statistics for all quantities (mean,sd,median,95%CI)
library(HDInterval)
#mean qol in trt0 non-zero
mean_qol_trt0_non0 <- round(mean(mm0.u.nonzero), 3)
sd_qol_trt0_non0 <- round(sd(mm0.u.nonzero), 2)
median_qol_trt0_non0 <- round(median(mm0.u.nonzero), 3)
CI95_qol_trt0_non0 <- round(hdi(mm0.u.nonzero, credMass = 0.95), 3)
#mean qol in trt1 non-zero
mean_qol_trt1_non0 <- round(mean(mm1.u.nonzero), 3)
sd_qol_trt1_non0 <- round(sd(mm1.u.nonzero), 2)
median_qol_trt1_non0 <- round(median(mm1.u.nonzero), 3)
CI95_qol_trt1_non0 <- round(hdi(mm1.u.nonzero, credMass = 0.95), 3)
#prob zero in trt0
mean_p_zero_trt0 <- round(mean(p0.zero), 3)
sd_p_zero_trt0 <- round(sd(p0.zero), 2)
median_p_zero_trt0 <- round(median(p0.zero), 3)
CI95_p_zero_trt0 <- round(hdi(p0.zero, credMass = 0.95), 3)
#prob zero in trt1
mean_p_zero_trt1 <- round(mean(p1.zero), 3)
sd_p_zero_trt1 <- round(sd(p1.zero), 2)
median_p_zero_trt1 <- round(median(p1.zero), 3)
CI95_p_zero_trt1 <- round(hdi(p1.zero, credMass = 0.95), 3)
#mean qol in trt0 all
mean_qol_trt0 <- round(mean(mm0.u.weighted), 3)
sd_qol_trt0 <- round(sd(mm0.u.weighted), 2)
median_qol_trt0 <- round(median(mm0.u.weighted), 3)
CI95_qol_trt0 <- round(hdi(mm0.u.weighted, credMass = 0.95), 3)
#mean qol in trt1 all
mean_qol_trt1 <- round(mean(mm1.u.weighted), 3)
sd_qol_trt1 <- round(sd(mm1.u.weighted), 2)
median_qol_trt1 <- round(median(mm1.u.weighted), 3)
CI95_qol_trt1 <- round(hdi(mm1.u.weighted, credMass = 0.95), 3)
#mean difference qol non-zero
mean_MD_qol_non0 <- round(mean(MD.u.nonzero), 3)
sd_MD_qol_non0 <- round(sd(MD.u.nonzero), 2)
median_MD_qol_non0 <- round(median(MD.u.nonzero), 3)
CI95_MD_qol_non0 <- round(hdi(MD.u.nonzero, credMass = 0.95), 3)
#ARD p zero
mean_ARD_p_zero <- round(mean(ARD.zero), 3)
sd_ARD_p_zero <- round(sd(ARD.zero), 2)
median_ARD_p_zero <- round(median(ARD.zero), 3)
CI95_ARD_p_zero <- round(hdi(ARD.zero, credMass = 0.95), 3)
#OR p zero
mean_OR_p_zero <- round(mean(OR.zero), 3)
sd_OR_p_zero <- round(sd(OR.zero), 2)
median_OR_p_zero <- round(median(OR.zero), 3)
CI95_OR_p_zero <- round(hdi(OR.zero, credMass = 0.95), 3)
#mean difference qol all
mean_MD_qol <- round(mean(MD.u.weighted), 3)
sd_MD_qol <- round(sd(MD.u.weighted), 2)
median_MD_qol <- round(median(MD.u.weighted), 3)
CI95_MD_qol <- round(hdi(MD.u.weighted, credMass = 0.95), 3)
#posterior probabilities in terms of qol MD across all
#any benefit P(MD>0)
prob_MD_qol_any_benefit <- sum(MD.u.weighted > 0)/length(MD.u.weighted)
#clinically important benefit P(MD>MCID)
mcid_qol <- c(0.06,0.04,0.05,0.07)
prob_MD_qol_MCID <- sum(MD.u.weighted > mcid_qol[1])/length(MD.u.weighted)
#clinically important harm P(MD> -MCID)
prob_MD_qol_harm <- sum(MD.u.weighted < -mcid_qol[1])/length(MD.u.weighted)

#repeat for other thresholds
prob_MD_qol_MCID4 <- sum(MD.u.weighted > mcid_qol[2])/length(MD.u.weighted)
prob_MD_qol_harm4 <- sum(MD.u.weighted < -mcid_qol[2])/length(MD.u.weighted)
prob_MD_qol_MCID5 <- sum(MD.u.weighted > mcid_qol[3])/length(MD.u.weighted)
prob_MD_qol_harm5 <- sum(MD.u.weighted < -mcid_qol[3])/length(MD.u.weighted)
prob_MD_qol_MCID7 <- sum(MD.u.weighted > mcid_qol[4])/length(MD.u.weighted)
prob_MD_qol_harm7 <- sum(MD.u.weighted < -mcid_qol[4])/length(MD.u.weighted)
```

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: fig-mdplot
#| fig-cap: "Posterior probability of having a value equal or lower than a range of effect sizes (upper plot) and posterior density for the estimated effect size (lower plot)."

#posterior distribution for adjusted mean difference in qol between arms
MD_qol_weight <- MD.u.weighted
#convert into data frame for plotting
MD_qol_weight.df <- data.frame(MD_qol_weight)

#posterior density of mean difference
#density estimation
library(tidyverse)
den <- density(MD_qol_weight, from = -0.1, to = 0.1)
data_dens_plot <- tibble(x = den$x, y = den$y) %>% 
    mutate(variable = case_when(
      (x >= mcid_qol[1]) ~ "On",
      TRUE ~ NA_character_))

md_plot1 <- ggplot(data_dens_plot, aes(x, y)) + geom_line(col="gold") +
  geom_area(data = filter(data_dens_plot, variable == 'On'), fill = 'gold') +
  theme_classic() + geom_vline(xintercept=0, col="black") +
  geom_vline(xintercept=mcid_qol[1], col="black", lty = 3) + 
  xlim(-0.1,0.1) + xlab("MD") + ylab("") +
  annotate("text", x = mcid_qol[1]+0.015, y = 5, label = "MCID") + 
  coord_cartesian(ylim = c(0, 30), clip = "off") + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))

#construct probability plot for given effect sizes
es <- seq(from=-0.1,to=0.1,by=0.005)
prob_es <- c()
for(i in 1:length(es)){
prob_es[i] <- sum(MD_qol_weight<es[i])/length(MD_qol_weight)
}
ES_qol.df <- data.frame(es,prob_es)


#posterior plot of probability of having effect size < given number
md_plot2 <- ggplot(ES_qol.df, aes(x=es, y=prob_es)) +
  geom_line(aes(x=es, y=prob_es), linewidth=0.8, colour='gold') + 
  theme_classic() + xlab("") + ylab("Prob (effect size < X)") +
  geom_vline(xintercept=0, col="black") +
  geom_vline(xintercept=mcid_qol[1], col="black", lty = 3) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))

ggarrange(md_plot2,md_plot1, nrow = 2, ncol = 1)

```

@fig-mdplot shows two graphs that summarise posterior inference about the effect size of interest, in this case the mean difference in EQ-5D utility score between the arms. The upper plot displays the probabilities associated with observing an estimated effect size equal to or lower than a given value (Y axis) for a range of possible effect size values (X axis). The second plot displays the full posterior density of the estimated effect size. In both plots a solid ad dashed lines are drawn in correspondence with a null effect size and an effect size equal to the MCID, respectively.

### Results from sensitivity analysis (skeptical prior)

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| cache-lazy: false

#update hyper prior values for skeptical version
prior_md_ms <- c(0,0.047) #means and sd for mean difference
prior_md_mp <- c(0,1/0.047^2) #means and precision for mean difference
zero_d <- dataset_long_eq5d$zero_d

#re-fit model with same file but with updated prior values
set.seed(3456)
datalist<-list("N","P","L","Nt","qol","qol_base","trt","time","id","zero_d","center","prior_lor_mp","prior_md_mp") 
inits <- function(){list()} 
params<-c("beta0","beta","sd.a0","sd.b0","mu.u","sd.u","pi","gamma0","gamma","qol_rep","zero_d_rep","zero_d","log_lik_qol","log_lik_zero_d","coef") #list of parameters to save
filein <- "model.mixture.qol.txt" #name of file with model
n.iter <- 10000 #number of iterations
n.chains <- 2 #number of chains
n.thin <- 1 #thinning interval

model_mixture_qol_ske <- jags(data=datalist,inits=inits,parameters.to.save=params,model.file=filein,n.chains=n.chains,n.iter=n.iter,n.thin=n.thin,DIC=TRUE)

model_mixture_qol_ske_sum <- round(jagsresults(x = model_mixture_qol_ske, params = c('mu.u','pi', 'zero_d_rep', 'qol_rep', 'zero_d','log_lik_qol', 'log_lik_zero_d','deviance','coef'), invert = TRUE), digits = 3)

#look at summary results
print(model_mixture_qol_ske_sum)
```

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false

#obtain all quantities of interest from the posterior estimates of the model via Monte Carlo integration

#compute marginal means qol for each trt and mean difference
mu.u_ske <- model_mixture_qol_ske$BUGSoutput$sims.list$mu.u
sd.u_ske <- model_mixture_qol_ske$BUGSoutput$sims.list$sd.u
zero_d_ske <- model_mixture_qol_ske$BUGSoutput$sims.list$zero_d
zero0_d_ske <- zero_d_ske[,trt==0]
zero1_d_ske <- zero_d_ske[,trt==1]
mu0.u_ske <- mu.u_ske[,trt==0]
mu1.u_ske <- mu.u_ske[,trt==1]
mu0_zero_d_ind_ske <- vector("list", n.iter)
mu1_zero_d_ind_ske <- vector("list", n.iter)
for(i in 1:n.iter){
  mu0_zero_d_ind_ske[[i]] <- which(zero0_d_ske[i,]==0)
  mu1_zero_d_ind_ske[[i]] <- which(zero1_d_ske[i,]==0)
}
pi0.zero_ske <- model_mixture_qol_ske$BUGSoutput$sims.list$pi[, trt==0]
pi1.zero_ske <- model_mixture_qol_ske$BUGSoutput$sims.list$pi[, trt==1]
mm0.u.nonzero_ske <- mm1.u.nonzero_ske <- c()
p0.zero_ske <- p1.zero_ske <- c()
#generate many samples and take expectation across replications at each MCMC iteration to approximate marginal parameter values 
set.seed(3456)
for(i in 1:n.iter){
  mm0.u.nonzero_ske[i] <- mean(rnorm(K, mean = mu0.u_ske[i, mu0_zero_d_ind_ske[[i]]], sd = sd.u_ske[i, 1]))
  mm1.u.nonzero_ske[i] <- mean(rnorm(K, mean = mu1.u_ske[i, mu1_zero_d_ind_ske[[i]]], sd = sd.u_ske[i, 1]))
  p0.zero_ske[i] <- mean(rbinom(K, size = 1, prob = pi0.zero_ske[i, ]))
  p1.zero_ske[i] <- mean(rbinom(K, size = 1, prob = pi1.zero_ske[i, ]))
}
#compute all other quantities desired
MD.u.nonzero_ske <-  mm1.u.nonzero_ske - mm0.u.nonzero_ske
ARD.zero_ske <-  p1.zero_ske - p0.zero_ske
mm0.u.weighted_ske <- mm0.u.nonzero_ske*(1-p0.zero_ske)
mm1.u.weighted_ske <- mm1.u.nonzero_ske*(1-p1.zero_ske)
MD.u.weighted_ske <- mm1.u.weighted_ske - mm0.u.weighted_ske
ODDS0.zero_ske <- p0.zero_ske/(1-p0.zero_ske)
ODDS1.zero_ske <- p1.zero_ske/(1-p1.zero_ske)
OR.zero_ske <- ODDS1.zero_ske/ODDS0.zero_ske

#obtain summary statistics for all quantities (mean,sd,median,95%CI)
#mean qol in trt0 non-zero
mean_qol_trt0_non0_ske <- round(mean(mm0.u.nonzero_ske), 3)
sd_qol_trt0_non0_ske <- round(sd(mm0.u.nonzero_ske), 2)
median_qol_trt0_non0_ske <- round(median(mm0.u.nonzero_ske), 3)
CI95_qol_trt0_non0_ske <- round(hdi(mm0.u.nonzero_ske, credMass = 0.95), 3)
#mean qol in trt1 non-zero
mean_qol_trt1_non0_ske <- round(mean(mm1.u.nonzero_ske), 3)
sd_qol_trt1_non0_ske <- round(sd(mm1.u.nonzero_ske), 2)
median_qol_trt1_non0_ske <- round(median(mm1.u.nonzero_ske), 3)
CI95_qol_trt1_non0_ske <- round(hdi(mm1.u.nonzero_ske, credMass = 0.95), 3)
#prob zero in trt0
mean_p_zero_trt0_ske <- round(mean(p0.zero_ske), 3)
sd_p_zero_trt0_ske <- round(sd(p0.zero_ske), 2)
median_p_zero_trt0_ske <- round(median(p0.zero_ske), 3)
CI95_p_zero_trt0_ske <- round(hdi(p0.zero_ske, credMass = 0.95), 3)
#prob zero in trt1
mean_p_zero_trt1_ske <- round(mean(p1.zero_ske), 3)
sd_p_zero_trt1_ske <- round(sd(p1.zero_ske), 2)
median_p_zero_trt1_ske <- round(median(p1.zero_ske), 3)
CI95_p_zero_trt1_ske <- round(hdi(p1.zero_ske, credMass = 0.95), 3)
#mean qol in trt0 all
mean_qol_trt0_ske <- round(mean(mm0.u.weighted_ske), 3)
sd_qol_trt0_ske <- round(sd(mm0.u.weighted_ske), 2)
median_qol_trt0_ske <- round(median(mm0.u.weighted_ske), 3)
CI95_qol_trt0_ske <- round(hdi(mm0.u.weighted_ske, credMass = 0.95), 3)
#mean qol in trt1 all
mean_qol_trt1_ske <- round(mean(mm1.u.weighted_ske), 3)
sd_qol_trt1_ske <- round(sd(mm1.u.weighted_ske), 2)
median_qol_trt1_ske <- round(median(mm1.u.weighted_ske), 3)
CI95_qol_trt1_ske <- round(hdi(mm1.u.weighted_ske, credMass = 0.95), 3)
#mean difference qol non-zero
mean_MD_qol_non0_ske <- round(mean(MD.u.nonzero_ske), 3)
sd_MD_qol_non0_ske <- round(sd(MD.u.nonzero_ske), 2)
median_MD_qol_non0_ske <- round(median(MD.u.nonzero_ske), 3)
CI95_MD_qol_non0_ske <- round(hdi(MD.u.nonzero_ske, credMass = 0.95), 3)
#ARD p zero
mean_ARD_p_zero_ske <- round(mean(ARD.zero_ske), 3)
sd_ARD_p_zero_ske <- round(sd(ARD.zero_ske), 2)
median_ARD_p_zero_ske <- round(median(ARD.zero_ske), 3)
CI95_ARD_p_zero_ske <- round(hdi(ARD.zero_ske, credMass = 0.95), 3)
#OR p zero
mean_OR_p_zero_ske <- round(mean(OR.zero_ske), 3)
sd_OR_p_zero_ske <- round(sd(OR.zero_ske), 2)
median_OR_p_zero_ske <- round(median(OR.zero_ske), 3)
CI95_OR_p_zero_ske <- round(hdi(OR.zero_ske, credMass = 0.95), 3)
#mean difference qol all
mean_MD_qol_ske <- round(mean(MD.u.weighted_ske), 3)
sd_MD_qol_ske <- round(sd(MD.u.weighted_ske), 2)
median_MD_qol_ske <- round(median(MD.u.weighted_ske), 3)
CI95_MD_qol_ske <- round(hdi(MD.u.weighted_ske, credMass = 0.95), 3)
#posterior probabilities in terms of qol MD across all
#any benefit P(MD>0)
prob_MD_qol_any_benefit_ske <- sum(MD.u.weighted_ske > 0)/length(MD.u.weighted_ske)
#clinically important benefit P(MD>MCID)
prob_MD_qol_MCID_ske <- sum(MD.u.weighted_ske > mcid_qol[1])/length(MD.u.weighted_ske)
#clinically important harm P(MD> -MCID)
prob_MD_qol_harm_ske <- sum(MD.u.weighted_ske < -mcid_qol[1])/length(MD.u.weighted_ske)

#repeat for other thresholds
prob_MD_qol_MCID4_ske <- sum(MD.u.weighted_ske > mcid_qol[2])/length(MD.u.weighted_ske)
prob_MD_qol_harm4_ske <- sum(MD.u.weighted_ske < -mcid_qol[2])/length(MD.u.weighted_ske)
prob_MD_qol_MCID5_ske <- sum(MD.u.weighted_ske > mcid_qol[3])/length(MD.u.weighted_ske)
prob_MD_qol_harm5_ske <- sum(MD.u.weighted_ske < -mcid_qol[3])/length(MD.u.weighted_ske)
prob_MD_qol_MCID7_ske <- sum(MD.u.weighted_ske > mcid_qol[4])/length(MD.u.weighted_ske)
prob_MD_qol_harm7_ske <- sum(MD.u.weighted_ske < -mcid_qol[4])/length(MD.u.weighted_ske)
```

We now inspect the posterior results for the parameters of interest. 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: fig-mdplot-ske
#| fig-cap: "Posterior probability of having a value equal or lower than a range of effect sizes (upper plot) and posterior density for the estimated effect size (lower plot)."

#posterior distribution for adjusted mean difference in qol between arms
MD_qol_weight_ske <- MD.u.weighted_ske
#convert into data frame for plotting
MD_qol_weight.df_ske <- data.frame(MD_qol_weight_ske)

#posterior density of mean difference
#density estimation
den_ske <- density(MD_qol_weight_ske, from = -0.1, to = 0.1)
data_dens_plot_ske <- tibble(x = den_ske$x, y = den_ske$y) %>% 
    mutate(variable = case_when(
      (x >= mcid_qol[1]) ~ "On",
      TRUE ~ NA_character_))

md_plot1_ske <- ggplot(data_dens_plot_ske, aes(x, y)) + geom_line(col="gold") +
  geom_area(data = filter(data_dens_plot_ske, variable == 'On'), fill = 'gold') +
  theme_classic() + geom_vline(xintercept=0, col="black") +
  geom_vline(xintercept=mcid_qol[1], col="black", lty = 3) + 
  xlim(-0.1,0.1) + xlab("MD") + ylab("") +
  annotate("text", x = mcid_qol[1]+0.015, y = 5, label = "MCID") + 
  coord_cartesian(ylim = c(0, 30), clip = "off") + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))

#construct probability plot for given effect sizes
es_ske <- seq(from=-0.1,to=0.1,by=0.005)
prob_es_ske <- c()
for(i in 1:length(es_ske)){
prob_es_ske[i] <- sum(MD_qol_weight_ske < es_ske[i])/length(MD_qol_weight_ske)
}
ES_qol.df_ske <- data.frame(es_ske,prob_es_ske)


#posterior plot of probability of having effect size < given number
md_plot2_ske <- ggplot(ES_qol.df_ske, aes(x=es_ske, y=prob_es_ske)) +
  geom_line(aes(x=es_ske, y=prob_es_ske), linewidth=0.8, colour='gold') + 
  theme_classic() + xlab("") + ylab("Prob (effect size < X)") +
  geom_vline(xintercept=0, col="black") +
  geom_vline(xintercept=mcid_qol[1], col="black", lty = 3) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))
  

ggarrange(md_plot2_ske,md_plot1_ske, nrow = 2, ncol = 1)

```

@fig-mdplot-ske shows two graphs that summarise posterior inference about the effect size of interest under the adoption of skeptical priors.

### Results from sensitivity analysis (enthusiastic prior)

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: true
#| cache-lazy: false

#update hyper prior values for skeptical version
prior_md_ms <- c(2*0.06,0.047) #means and sd for mean difference
prior_md_mp <- c(2*0.06,1/0.047^2) #means and precision for mean difference

#re-fit model with same file but with updated prior values
set.seed(3456)
datalist<-list("N","P","L","Nt","qol","qol_base","trt","time","id","zero_d","center","prior_lor_mp","prior_md_mp") 
inits <- function(){list()} 
params<-c("beta0","beta","sd.a0","sd.b0","mu.u","sd.u","pi","gamma0","gamma","qol_rep","zero_d_rep","zero_d","log_lik_qol","log_lik_zero_d","coef") #list of parameters to save
filein <- "model.mixture.qol.txt" #name of file with model
n.iter <- 10000 #number of iterations
n.chains <- 2 #number of chains
n.thin <- 1 #thinning interval

model_mixture_qol_ent <- jags(data=datalist,inits=inits,parameters.to.save=params,model.file=filein,n.chains=n.chains,n.iter=n.iter,n.thin=n.thin,DIC=TRUE)

model_mixture_qol_ent_sum <- round(jagsresults(x = model_mixture_qol_ent, params = c('mu.u','pi', 'zero_d_rep', 'qol_rep', 'zero_d','log_lik_qol', 'log_lik_zero_d','deviance','coef'), invert = TRUE), digits = 3)

#look at summary results
print(model_mixture_qol_ent_sum)
```

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false

#obtain all quantities of interest from the posterior estimates of the model via Monte Carlo integration

#compute marginal means qol for each trt and mean difference
mu.u_ent <- model_mixture_qol_ent$BUGSoutput$sims.list$mu.u
sd.u_ent <- model_mixture_qol_ent$BUGSoutput$sims.list$sd.u
zero_d_ent <- model_mixture_qol_ent$BUGSoutput$sims.list$zero_d
zero0_d_ent <- zero_d_ent[,trt==0]
zero1_d_ent <- zero_d_ent[,trt==1]
mu0.u_ent <- mu.u_ent[,trt==0]
mu1.u_ent <- mu.u_ent[,trt==1]
mu0_zero_d_ind_ent <- vector("list", n.iter)
mu1_zero_d_ind_ent <- vector("list", n.iter)
for(i in 1:n.iter){
  mu0_zero_d_ind_ent[[i]] <- which(zero0_d_ent[i,]==0)
  mu1_zero_d_ind_ent[[i]] <- which(zero1_d_ent[i,]==0)
}
pi0.zero_ent <- model_mixture_qol_ent$BUGSoutput$sims.list$pi[, trt==0]
pi1.zero_ent <- model_mixture_qol_ent$BUGSoutput$sims.list$pi[, trt==1]
mm0.u.nonzero_ent <- mm1.u.nonzero_ent <- c()
p0.zero_ent <- p1.zero_ent <- c()
#generate many samples and take expectation across replications at each MCMC iteration to approximate marginal parameter values 
set.seed(3456)
for(i in 1:n.iter){
  mm0.u.nonzero_ent[i] <- mean(rnorm(K, mean = mu0.u_ent[i, mu0_zero_d_ind_ent[[i]]], sd = sd.u_ent[i, 1]))
  mm1.u.nonzero_ent[i] <- mean(rnorm(K, mean = mu1.u_ent[i, mu1_zero_d_ind_ent[[i]]], sd = sd.u_ent[i, 1]))
  p0.zero_ent[i] <- mean(rbinom(K, size = 1, prob = pi0.zero_ent[i, ]))
  p1.zero_ent[i] <- mean(rbinom(K, size = 1, prob = pi1.zero_ent[i, ]))
}
#compute all other quantities desired
MD.u.nonzero_ent <-  mm1.u.nonzero_ent - mm0.u.nonzero_ent
ARD.zero_ent <-  p1.zero_ent - p0.zero_ent
mm0.u.weighted_ent <- mm0.u.nonzero_ent*(1-p0.zero_ent)
mm1.u.weighted_ent <- mm1.u.nonzero_ent*(1-p1.zero_ent)
MD.u.weighted_ent <- mm1.u.weighted_ent - mm0.u.weighted_ent
ODDS0.zero_ent <- p0.zero_ent/(1-p0.zero_ent)
ODDS1.zero_ent <- p1.zero_ent/(1-p1.zero_ent)
OR.zero_ent <- ODDS1.zero_ent/ODDS0.zero_ent

#obtain summary statistics for all quantities (mean,sd,median,95%CI)
#mean qol in trt0 non-zero
mean_qol_trt0_non0_ent <- round(mean(mm0.u.nonzero_ent), 3)
sd_qol_trt0_non0_ent <- round(sd(mm0.u.nonzero_ent), 2)
median_qol_trt0_non0_ent <- round(median(mm0.u.nonzero_ent), 3)
CI95_qol_trt0_non0_ent <- round(hdi(mm0.u.nonzero_ent, credMass = 0.95), 3)
#mean qol in trt1 non-zero
mean_qol_trt1_non0_ent <- round(mean(mm1.u.nonzero_ent), 3)
sd_qol_trt1_non0_ent <- round(sd(mm1.u.nonzero_ent), 2)
median_qol_trt1_non0_ent <- round(median(mm1.u.nonzero_ent), 3)
CI95_qol_trt1_non0_ent <- round(hdi(mm1.u.nonzero_ent, credMass = 0.95), 3)
#prob zero in trt0
mean_p_zero_trt0_ent <- round(mean(p0.zero_ent), 3)
sd_p_zero_trt0_ent <- round(sd(p0.zero_ent), 2)
median_p_zero_trt0_ent <- round(median(p0.zero_ent), 3)
CI95_p_zero_trt0_ent <- round(hdi(p0.zero_ent, credMass = 0.95), 3)
#prob zero in trt1
mean_p_zero_trt1_ent <- round(mean(p1.zero_ent), 3)
sd_p_zero_trt1_ent <- round(sd(p1.zero_ent), 2)
median_p_zero_trt1_ent <- round(median(p1.zero_ent), 3)
CI95_p_zero_trt1_ent <- round(hdi(p1.zero_ent, credMass = 0.95), 3)
#mean qol in trt0 all
mean_qol_trt0_ent <- round(mean(mm0.u.weighted_ent), 3)
sd_qol_trt0_ent <- round(sd(mm0.u.weighted_ent), 2)
median_qol_trt0_ent <- round(median(mm0.u.weighted_ent), 3)
CI95_qol_trt0_ent <- round(hdi(mm0.u.weighted_ent, credMass = 0.95), 3)
#mean qol in trt1 all
mean_qol_trt1_ent <- round(mean(mm1.u.weighted_ent), 3)
sd_qol_trt1_ent <- round(sd(mm1.u.weighted_ent), 2)
median_qol_trt1_ent <- round(median(mm1.u.weighted_ent), 3)
CI95_qol_trt1_ent <- round(hdi(mm1.u.weighted_ent, credMass = 0.95), 3)
#mean difference qol non-zero
mean_MD_qol_non0_ent <- round(mean(MD.u.nonzero_ent), 3)
sd_MD_qol_non0_ent <- round(sd(MD.u.nonzero_ent), 2)
median_MD_qol_non0_ent <- round(median(MD.u.nonzero_ent), 3)
CI95_MD_qol_non0_ent <- round(hdi(MD.u.nonzero_ent, credMass = 0.95), 3)
#ARD p zero
mean_ARD_p_zero_ent <- round(mean(ARD.zero_ent), 3)
sd_ARD_p_zero_ent <- round(sd(ARD.zero_ent), 2)
median_ARD_p_zero_ent <- round(median(ARD.zero_ent), 3)
CI95_ARD_p_zero_ent <- round(hdi(ARD.zero_ent, credMass = 0.95), 3)
#OR p zero
mean_OR_p_zero_ent <- round(mean(OR.zero_ent), 3)
sd_OR_p_zero_ent <- round(sd(OR.zero_ent), 2)
median_OR_p_zero_ent <- round(median(OR.zero_ent), 3)
CI95_OR_p_zero_ent <- round(hdi(OR.zero_ent, credMass = 0.95), 3)
#mean difference qol all
mean_MD_qol_ent <- round(mean(MD.u.weighted_ent), 3)
sd_MD_qol_ent <- round(sd(MD.u.weighted_ent), 2)
median_MD_qol_ent <- round(median(MD.u.weighted_ent), 3)
CI95_MD_qol_ent <- round(hdi(MD.u.weighted_ent, credMass = 0.95), 3)
#posterior probabilities in terms of qol MD across all
#any benefit P(MD>0)
prob_MD_qol_any_benefit_ent <- sum(MD.u.weighted_ent > 0)/length(MD.u.weighted_ent)
#clinically important benefit P(MD>MCID)
prob_MD_qol_MCID_ent <- sum(MD.u.weighted_ent > mcid_qol[1])/length(MD.u.weighted_ent)
#clinically important harm P(MD> -MCID)
prob_MD_qol_harm_ent <- sum(MD.u.weighted_ent < -mcid_qol[1])/length(MD.u.weighted_ent)

#repeat for other thresholds
prob_MD_qol_MCID4_ent <- sum(MD.u.weighted_ent > mcid_qol[2])/length(MD.u.weighted_ent)
prob_MD_qol_harm4_ent <- sum(MD.u.weighted_ent < -mcid_qol[2])/length(MD.u.weighted_ent)
prob_MD_qol_MCID5_ent <- sum(MD.u.weighted_ent > mcid_qol[3])/length(MD.u.weighted_ent)
prob_MD_qol_harm5_ent <- sum(MD.u.weighted_ent < -mcid_qol[3])/length(MD.u.weighted_ent)
prob_MD_qol_MCID7_ent <- sum(MD.u.weighted_ent > mcid_qol[4])/length(MD.u.weighted_ent)
prob_MD_qol_harm7_ent <- sum(MD.u.weighted_ent < -mcid_qol[4])/length(MD.u.weighted_ent)
```

We now inspect the posterior results for the parameters of interest. 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: fig-mdplot-ent
#| fig-cap: "Posterior probability of having a value equal or lower than a range of effect sizes (upper plot) and posterior density for the estimated effect size (lower plot)."

#posterior distribution for adjusted mean difference in qol between arms
MD_qol_weight_ent <- MD.u.weighted_ent
#convert into data frame for plotting
MD_qol_weight.df_ent <- data.frame(MD_qol_weight_ent)

#posterior density of mean difference
#density estimation
den_ent <- density(MD_qol_weight_ent, from = -0.1, to = 0.1)
data_dens_plot_ent <- tibble(x = den_ent$x, y = den_ent$y) %>% 
    mutate(variable = case_when(
      (x >= mcid_qol[1]) ~ "On",
      TRUE ~ NA_character_))

md_plot1_ent <- ggplot(data_dens_plot_ent, aes(x, y)) + geom_line(col="gold") +
  geom_area(data = filter(data_dens_plot_ent, variable == 'On'), fill = 'gold') +
  theme_classic() + geom_vline(xintercept=0, col="black") +
  geom_vline(xintercept=mcid_qol[1], col="black", lty = 3) + 
  xlim(-0.1,0.1) + xlab("MD") + ylab("") +
  annotate("text", x = mcid_qol[1], y = 5, label = "MCID") + 
  coord_cartesian(ylim = c(0, 30), clip = "off") + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))

#construct probability plot for given effect sizes
es_ent <- seq(from=-0.1,to=0.1,by=0.005)
prob_es_ent <- c()
for(i in 1:length(es_ent)){
prob_es_ent[i] <- sum(MD_qol_weight_ent < es_ent[i])/length(MD_qol_weight_ent)
}
ES_qol.df_ent <- data.frame(es_ent,prob_es_ent)


#posterior plot of probability of having effect size < given number
md_plot2_ent <- ggplot(ES_qol.df_ent, aes(x=es_ent, y=prob_es_ent)) +
  geom_line(aes(x=es_ent, y=prob_es_ent), linewidth=0.8, colour='gold') + 
  theme_classic() + xlab("") + ylab("Prob (effect size < X)") +
  geom_vline(xintercept=0, col="black") +
  geom_vline(xintercept=mcid_qol[1], col="black", lty = 3) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA))
  

ggarrange(md_plot2_ent,md_plot1_ent, nrow = 2, ncol = 1)

```

@fig-mdplot-ent shows two graphs that summarise posterior inference about the effect size of interest under the adoption of enthusiastic priors.

### Summary posterior results

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false

#put summary statistics for posterior estimates of effect size for each model in tables
tab_qol_res <- matrix(NA,13,3)
rownames(tab_qol_res) <- c("mean(95%CI) qol non-zero","mean(95%CI) p zero","mean(95%CI) odds zero","mean(95%CI) qol","P(>0)","P(>MCID=0.06)","P(>-MCID=0.06)","P(>MCID=0.04)","P(>-MCID=0.04)","P(>MCID=0.05)","P(>-MCID=0.05)","P(>MCID=0.07)","P(>-MCID=0.07)")
colnames(tab_qol_res) <- c("standard protein","high protein","difference")

tab_qol_res[1,] <- c(paste(mean_qol_trt0_non0, "(",CI95_qol_trt0_non0[1],";",CI95_qol_trt0_non0[2],")", sep = ""),paste(mean_qol_trt1_non0,"(",CI95_qol_trt1_non0[1],";",CI95_qol_trt1_non0[2],")", sep = ""),paste(mean_MD_qol_non0,"(",CI95_MD_qol_non0[1],";",CI95_MD_qol_non0[2],")", sep = ""))
tab_qol_res[2,] <- c(paste(mean_p_zero_trt0, "(",CI95_p_zero_trt0[1],";",CI95_p_zero_trt0[2],")", sep = ""),paste(mean_p_zero_trt1,"(",CI95_p_zero_trt1[1],";",CI95_p_zero_trt1[2],")", sep = ""),paste(mean_ARD_p_zero,"(",CI95_ARD_p_zero[1],";",CI95_ARD_p_zero[2],")", sep = ""))
tab_qol_res[3,] <- c("","",paste(mean_OR_p_zero,"(",CI95_OR_p_zero[1],";",CI95_OR_p_zero[2],")", sep = ""))
tab_qol_res[4,] <- c(paste(mean_qol_trt0, "(",CI95_qol_trt0[1],";",CI95_qol_trt0[2],")", sep = ""),paste(mean_qol_trt1,"(",CI95_qol_trt1[1],";",CI95_qol_trt1[2],")", sep = ""),paste(mean_MD_qol,"(",CI95_MD_qol[1],";",CI95_MD_qol[2],")", sep = ""))
tab_qol_res[5,] <- c("","",prob_MD_qol_any_benefit)
tab_qol_res[6,] <- c("","",prob_MD_qol_MCID)
tab_qol_res[7,] <- c("","",prob_MD_qol_harm)
tab_qol_res[8,] <- c("","",prob_MD_qol_MCID4)
tab_qol_res[9,] <- c("","",prob_MD_qol_harm4)
tab_qol_res[10,] <- c("","",prob_MD_qol_MCID5)
tab_qol_res[11,] <- c("","",prob_MD_qol_harm5)
tab_qol_res[12,] <- c("","",prob_MD_qol_MCID7)
tab_qol_res[13,] <- c("","",prob_MD_qol_harm7)

tab_qol_res_ske <- matrix(NA,13,3)
rownames(tab_qol_res_ske) <- c("mean(95%CI) qol non-zero","mean(95%CI) p zero","mean(95%CI) odds zero","mean(95%CI) qol","P(>0)","P(>MCID=0.06)","P(>-MCID=0.06)","P(>MCID=0.04)","P(>-MCID=0.04)","P(>MCID=0.05)","P(>-MCID=0.05)","P(>MCID=0.07)","P(>-MCID=0.07)")
colnames(tab_qol_res_ske) <- c("standard protein","high protein","difference")
tab_qol_res_ske[1,] <- c(paste(mean_qol_trt0_non0_ske, "(",CI95_qol_trt0_non0_ske[1],";",CI95_qol_trt0_non0_ske[2],")", sep = ""),paste(mean_qol_trt1_non0_ske,"(",CI95_qol_trt1_non0_ske[1],";",CI95_qol_trt1_non0_ske[2],")", sep = ""),paste(mean_MD_qol_non0_ske,"(",CI95_MD_qol_non0_ske[1],";",CI95_MD_qol_non0_ske[2],")", sep = ""))
tab_qol_res_ske[2,] <- c(paste(mean_p_zero_trt0_ske, "(",CI95_p_zero_trt0_ske[1],";",CI95_p_zero_trt0_ske[2],")", sep = ""),paste(mean_p_zero_trt1_ske,"(",CI95_p_zero_trt1_ske[1],";",CI95_p_zero_trt1_ske[2],")", sep = ""),paste(mean_ARD_p_zero_ske,"(",CI95_ARD_p_zero_ske[1],";",CI95_ARD_p_zero_ske[2],")", sep = ""))
tab_qol_res_ske[3,] <- c("","",paste(mean_OR_p_zero_ske,"(",CI95_OR_p_zero_ske[1],";",CI95_OR_p_zero_ske[2],")", sep = ""))
tab_qol_res_ske[4,] <- c(paste(mean_qol_trt0_ske, "(",CI95_qol_trt0_ske[1],";",CI95_qol_trt0_ske[2],")", sep = ""),paste(mean_qol_trt1_ske,"(",CI95_qol_trt1_ske[1],";",CI95_qol_trt1_ske[2],")", sep = ""),paste(mean_MD_qol_ske,"(",CI95_MD_qol_ske[1],";",CI95_MD_qol_ske[2],")", sep = ""))
tab_qol_res_ske[5,] <- c("","",prob_MD_qol_any_benefit_ske)
tab_qol_res_ske[6,] <- c("","",prob_MD_qol_MCID_ske)
tab_qol_res_ske[7,] <- c("","",prob_MD_qol_harm_ske)
tab_qol_res_ske[8,] <- c("","",prob_MD_qol_MCID4_ske)
tab_qol_res_ske[9,] <- c("","",prob_MD_qol_harm4_ske)
tab_qol_res_ske[10,] <- c("","",prob_MD_qol_MCID5_ske)
tab_qol_res_ske[11,] <- c("","",prob_MD_qol_harm5_ske)
tab_qol_res_ske[12,] <- c("","",prob_MD_qol_MCID7_ske)
tab_qol_res_ske[13,] <- c("","",prob_MD_qol_harm7_ske)

tab_qol_res_ent <- matrix(NA,13,3)
rownames(tab_qol_res_ent) <- c("mean(95%CI) qol non-zero","mean(95%CI) p zero","mean(95%CI) odds zero","mean(95%CI) qol","P(>0)","P(>MCID=0.06)","P(>-MCID=0.06)","P(>MCID=0.04)","P(>-MCID=0.04)","P(>MCID=0.05)","P(>-MCID=0.05)","P(>MCID=0.07)","P(>-MCID=0.07)")
colnames(tab_qol_res_ent) <- c("standard protein","high protein","difference")
tab_qol_res_ent[1,] <- c(paste(mean_qol_trt0_non0_ent, "(",CI95_qol_trt0_non0_ent[1],";",CI95_qol_trt0_non0_ent[2],")", sep = ""),paste(mean_qol_trt1_non0_ent,"(",CI95_qol_trt1_non0_ent[1],";",CI95_qol_trt1_non0_ent[2],")", sep = ""),paste(mean_MD_qol_non0_ent,"(",CI95_MD_qol_non0_ent[1],";",CI95_MD_qol_non0_ent[2],")", sep = ""))
tab_qol_res_ent[2,] <- c(paste(mean_p_zero_trt0_ent, "(",CI95_p_zero_trt0_ent[1],";",CI95_p_zero_trt0_ent[2],")", sep = ""),paste(mean_p_zero_trt1_ent,"(",CI95_p_zero_trt1_ent[1],";",CI95_p_zero_trt1_ent[2],")", sep = ""),paste(mean_ARD_p_zero_ent,"(",CI95_ARD_p_zero_ent[1],";",CI95_ARD_p_zero_ent[2],")", sep = ""))
tab_qol_res_ent[3,] <- c("","",paste(mean_OR_p_zero_ent,"(",CI95_OR_p_zero_ent[1],";",CI95_OR_p_zero_ent[2],")", sep = ""))
tab_qol_res_ent[4,] <- c(paste(mean_qol_trt0_ent, "(",CI95_qol_trt0_ent[1],";",CI95_qol_trt0_ent[2],")", sep = ""),paste(mean_qol_trt1_ent,"(",CI95_qol_trt1_ent[1],";",CI95_qol_trt1_ent[2],")", sep = ""),paste(mean_MD_qol_ent,"(",CI95_MD_qol_ent[1],";",CI95_MD_qol_ent[2],")", sep = ""))
tab_qol_res_ent[5,] <- c("","",prob_MD_qol_any_benefit_ent)
tab_qol_res_ent[6,] <- c("","",prob_MD_qol_MCID_ent)
tab_qol_res_ent[7,] <- c("","",prob_MD_qol_harm_ent)
tab_qol_res_ent[8,] <- c("","",prob_MD_qol_MCID4_ent)
tab_qol_res_ent[9,] <- c("","",prob_MD_qol_harm4_ent)
tab_qol_res_ent[10,] <- c("","",prob_MD_qol_MCID5_ent)
tab_qol_res_ent[11,] <- c("","",prob_MD_qol_harm5_ent)
tab_qol_res_ent[12,] <- c("","",prob_MD_qol_MCID7_ent)
tab_qol_res_ent[13,] <- c("","",prob_MD_qol_harm7_ent)
```

@tbl-res1, @tbl-res2 and @tbl-res3 show key posterior summaries about the main quantities of interest under a weakly-informative, skeptical and enthusiastic prior specification, respectively. Posterior estimates are reported in terms of mean/median and $95\%$ credible intervals for each arm separately and in terms of MD/ARD/OR and probability of having any benefit, a clinically important benefit and harm between the two arms.

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-res1
#| tbl-cap: "Summary posterior estimates based on the model under weakly-informative priors: these include the mean of non-zero utility, probability of having a zero utility, mean utility by treatment arm. In addition, estimates of key differential quantities between arms are reported: MD among non-zero utility, ARD and OR for having a zero utility, MD utility,probability of having any benefit, a clinically important benefit and harm."

#print table of results
library(kableExtra)
kbl(tab_qol_res, booktabs=TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  scroll_box(width = "100%", height = "300px")

```


```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-res2
#| tbl-cap: "Summary posterior estimates based on the model under skeptical priors: these include the mean of non-zero utility, probability of having a zero utility, mean utility by treatment arm. In addition, estimates of key differential quantities between arms are reported: MD among non-zero utility, ARD and OR for having a zero utility, MD utility,probability of having any benefit, a clinically important benefit and harm."

#print table of results
kbl(tab_qol_res_ske, booktabs=TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  scroll_box(width = "100%", height = "300px")

```


```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false
#| label: tbl-res3
#| tbl-cap: "Summary posterior estimates based on the model under enthusiastic priors: these include the mean of non-zero utility, probability of having a zero utility, mean utility by treatment arm. In addition, estimates of key differential quantities between arms are reported: MD among non-zero utility, ARD and OR for having a zero utility, MD utility,probability of having any benefit, a clinically important benefit and harm."

#print table of results
kbl(tab_qol_res_ent, booktabs=TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  scroll_box(width = "100%", height = "300px")

```

